{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed83cf5d-d3da-45e0-8be9-9142bb2e27fc",
   "metadata": {},
   "source": [
    "## Read file with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "888f8e65-d175-4b69-8a31-da807682baee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ethereal', 'woebegone', 'credulous']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('unique_english_words.txt', 'r') as f:\n",
    "    words = [word.rstrip() for word in f.readlines()] \n",
    "\n",
    "words[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5082c84-bd85-435e-b0db-b79fbd835038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words: 750\n",
      "Min len: 3\n",
      "Max len: 29\n"
     ]
    }
   ],
   "source": [
    "print(f'No of words: {len(words)}')\n",
    "print(f'Min len: {min([len(word) for word in words])}')\n",
    "print(f'Max len: {max([len(word) for word in words])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bcb49-ff6c-4705-a74f-3ff17c2c7a98",
   "metadata": {},
   "source": [
    "## Creating bigram using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45555cc7-fb97-43d8-b438-6a9e7364026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c27c3c2-2b4f-476a-9f83-b0242f4b812a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_symbols = set(list(''.join(words)))\n",
    "unique_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2769476e-0112-40f4-9c83-97789a4fbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN = '.'\n",
    "unique_symbols.add(SPECIAL_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2da640fa-a3e5-4d06-9366-7140d53a6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s: i for i, s in enumerate(unique_symbols)}\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e19ada7-98b7-4a01-969b-16038e583341",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = len(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ad4ec0f-0336-4f34-9faa-0292e7d05642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   4,   0,   0,   2,   0,   0,   0,   0,   1,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   7,   0,   3,   0,   0,   0],\n",
       "        [  0,  10,  29,   0,   1,  31,   0,   0,   4,   1,  59,   1,  39,  22,\n",
       "           0,   2,   1,  31,   2,   8,   0,  55,  22,   1,  75,   0,   3,   0],\n",
       "        [  2,  75,   1,  24,  30,   0,   4,   0,  63,   6,   4,  85,   1,   4,\n",
       "          28,   7,   0,  51,  26,  36,  19,  39,  13,  27,  13,   4,   6,   0],\n",
       "        [  0,   0,  19,   4,   1,  25,   0,   0,  11,   0,  20,   1,   0,   2,\n",
       "           1,   0,   0,  12,   4,   0,   0,   4,  16,   0,  24,   0,   2,   0],\n",
       "        [  0,   0,  34,  14,   6,  24,   0,   1,   1,   0,  22,   2,   0,   3,\n",
       "           2,   0,   0,   0,   2,   0,  20,  23,  17,   0,  25,   0,   3,   0],\n",
       "        [  0,  11,   5,  18,  23,  26,   3,   2,  45,   7,   5,  67,   1,   2,\n",
       "          11,   6,   0,  43,  15,  12,  34,  18,  80,  12,   5,   6,   6,   4],\n",
       "        [  0,   0,  11,   0,   0,   7,   0,   0,   0,   0,  19,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   3,   0,  11,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,  25,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,  57,   5,   0,  39,   1,   0,  47,   0,  57,   1,   1,  13,\n",
       "           2,   0,   0,   0,   1,   2,   1,  38,  27,   3,  58,   2,   3,   0],\n",
       "        [  0,   0,  11,   1,   0,   6,   0,   0,   0,   0,   2,   3,   8,   1,\n",
       "           1,   0,   0,   0,   2,   0,   0,   5,   2,   0,  13,   0,   1,   0],\n",
       "        [  2,  33,  16,   8,  20,   8,   6,   3,  38,   4,  11,  82,   2,   7,\n",
       "          23,   3,   4, 105,  43,  15,  11, 129,  11,   6,   6,   4,  10,  11],\n",
       "        [  0,  61,  32,   1,   0,  22,   1,   3,   0,   0,  35,   6,   0,   4,\n",
       "          34,   2,   1,   3,  16,  32,   2,  62,   8,  37,  31,   7,   4,   0],\n",
       "        [  0,   5,  36,   0,   3,  32,   0,   0,   1,   1,  30,   1,   0,   3,\n",
       "           0,   0,   0,   8,   0,   1,   0,  14,   6,   0,  22,   1,   0,   0],\n",
       "        [  0,   1,   5,   2,   4,   3,   0,   0,   6,   1,   2,   2,   0,   0,\n",
       "           2,   2,   2,   4,   5,   1,   5,  75,   1,   6,   0,   0,   0,   0],\n",
       "        [  0,   0,  17,   3,   2,  21,   0,   0,   9,   1,  49,   0,   0,   7,\n",
       "          10,   1,   1,   5,   2,   0,   0,  32,   7,   4,  35,   0,   1,   0],\n",
       "        [  0,   0,   7,   0,   2,   7,   1,   0,   1,   0,  14,   0,   0,   3,\n",
       "           0,   5,   0,   0,   0,   0,   0,   4,   0,   1,   5,   0,   0,   0],\n",
       "        [  0,   1,   1,   1,   0,   0,   0,   0,   0,   0,   1,   1,   0,   0,\n",
       "           1,   0,   0,   1,   0,   0,   1,   0,   0,   1,   0,   1,   0,   0],\n",
       "        [  1,   8,  67,   4,   7,  40,   6,   0,   5,   2,  49,   8,   3,  18,\n",
       "           7,   0,   0,   9,   5,   7,   3,  48,  20,  13,  47,   4,   4,   0],\n",
       "        [  0,  48,  23,   2,  14,  14,   0,   4,   2,   1,  24,   2,  17,   8,\n",
       "           1,   0,   1,   0,  19,  36,  18, 104,  18,   0,  26,  11,   1,   0],\n",
       "        [  0,  10,  45,   0,   0,  36,   0,   0,   4,   0,  42,   0,  35,   6,\n",
       "           1,   0,   0,  14,   0,   3,   0,  32,  13,   0,  30,  19,   0,   0],\n",
       "        [  0,  13,  20,   0,   0,  16,   0,   0,  14,   0,  37,   1,  25,   4,\n",
       "           1,   0,   0,  12,   7,   0,   9,   5,   6,   0,  18,   0,   0,   0],\n",
       "        [ 12,  35,  31,  41,  36,  22,  17,   8,  18,  28,  63,  16,  26,   2,\n",
       "          45,  14,   0,  26,  97,  65,  38,   0,  13,  21,  26,  11,  33,   6],\n",
       "        [  0,  18,   5,  10,  32,  14,   2,   0,  30,   0,   9,  23,   0,   2,\n",
       "          15,   2,   0,  32,  93,  12,   9,   3,   2,   8,  21,   2,   6,   3],\n",
       "        [  0,   1,  31,   1,   6,  13,   0,   0,  12,   2,  17,   6,   4,   9,\n",
       "           1,   0,   0,   8,   1,   0,   0,  27,  14,   7,  17,   0,   0,   0],\n",
       "        [  0,  57,  36,   7,  12,  49,   9,   3,  30,   2,  13,  95,   2,   0,\n",
       "          19,   6,   0,  11,  51,  60,  18,   7,   2,  30,   0,   2,  12,   3],\n",
       "        [  0,   1,   7,   0,   0,   1,   1,   0,   2,   0,  18,   1,   0,   6,\n",
       "           2,   0,   0,   1,   2,   0,   0,  15,   4,   0,  13,   0,   0,   0],\n",
       "        [  0,   3,  17,   0,   0,   7,   0,   0,  15,   0,  13,   0,   0,   0,\n",
       "           0,   0,   0,   9,   0,   0,   0,   6,  15,   0,  10,   0,  20,   0],\n",
       "        [  0,   6,   1,   0,   0,   2,   0,   1,   0,   0,   5,   0,   0,   3,\n",
       "           0,   0,   0,   0,   1,   0,   0,   5,   1,   0,   2,   0,   0,   0]],\n",
       "       dtype=torch.int16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros((n_symbols, n_symbols), dtype = torch.int16)\n",
    "\n",
    "for word in words:\n",
    "    word = SPECIAL_TOKEN + word + SPECIAL_TOKEN # Add special token in the begginning and in the end of each word\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "\n",
    "        N[idx1, idx2] += 1\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4bea2ad-b3c6-4a97-b1cd-d31d389bbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new tensor P to store probabilities\n",
    "P = (N + 1).float() ## adding fake 1 count to avoid log(0) sutiation during loss calculation!\n",
    "P = P/P.sum(dim = 1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "838382f8-a944-4575-848d-826c5310141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ol\n",
      "uzentis\n",
      "ngtona\n",
      "a\n",
      "prvemave-m\n",
      "dwhicoumddopalohes\n",
      "jwg\n",
      "nss\n",
      "fqalivon\n",
      "pur\n",
      "o\n",
      "s\n",
      "cemafldesas\n",
      "cror\n",
      "talogammsticr\n",
      "wwichuongillide\n",
      "cty\n",
      "mkaro\n",
      "h\n",
      "holiterdeene\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42123)\n",
    "\n",
    "# let's generate some new unique English words!\n",
    "for i in range(20):\n",
    "    word = []\n",
    "    idx = stoi[SPECIAL_TOKEN] # any word always starts with special token '.'\n",
    "    while True:\n",
    "        i_probs = P[idx]\n",
    "        idx = torch.multinomial(i_probs, num_samples = 1, replacement = True, generator = g).item()\n",
    "        if idx == stoi[SPECIAL_TOKEN]: # if we meet special token - it is the end of the word!\n",
    "            break  \n",
    "        word.append(itos[idx])\n",
    "\n",
    "    print(''.join(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f29d4b-00d2-4440-94b1-a44d1e8f5539",
   "metadata": {},
   "source": [
    "## How good is this bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60db5584-88e0-466c-a2f5-530ecce254ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e : 0.08\n",
      "et : 0.05\n",
      "th : 0.09\n",
      "he : 0.16\n",
      "er : 0.16\n",
      "re : 0.12\n",
      "ea : 0.03\n",
      "al : 0.11\n",
      "l. : 0.10\n",
      ".w : 0.04\n",
      "wo : 0.08\n",
      "oe : 0.01\n",
      "eb : 0.01\n",
      "be : 0.12\n",
      "eg : 0.01\n",
      "go : 0.07\n",
      "on : 0.14\n",
      "ne : 0.08\n",
      "e. : 0.20\n"
     ]
    }
   ],
   "source": [
    "# take first two words and check probabilities for pair of characters\n",
    "\n",
    "for word in words[:2]:\n",
    "    word = SPECIAL_TOKEN + word + SPECIAL_TOKEN\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "\n",
    "        print(f'{ch1}{ch2} : {P[idx1, idx2]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a73d987-e1a8-4645-86cd-6d6cb19797f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If every out of 28 (27+special token) chars were equaly likely then the probability for each pair of chars would be: 0.04.\n"
     ]
    }
   ],
   "source": [
    "print(f'If every out of 28 (27+special token) chars were equaly likely \\\n",
    "then the probability for each pair of chars would be: {1/28.:.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e924b7f-6839-4ec7-bcdb-757abc60bfe1",
   "metadata": {},
   "source": [
    "But we can see above that some of the probabilities are higher than 4% (eg \"he : 0.18\" or \"e. : 0.21\").\n",
    "<br>It means that our bigram model actually learnt something.\n",
    "<br><br>But how to quantify this in a single number which would expess the quality of our model?\n",
    "<br>To do this we can use likelihood which can be calculated as `a product of these probabilities`.\n",
    "<br>`The better the model` we have `the greater the product` value we should get because in a good model all these probabilities should be near 1.\n",
    "<br>But because all prob values are between 0 and 1 `their product is gonna be a very small number` which is not convinient.\n",
    "<br>So for convinience what is usually used is **LOG-LIKELIHOOD**.\n",
    "<br>But it is unnecessary to calculate this tiny product first ang then log(product). \n",
    "<br>Logarithm has such a nice property as: `log(a*b*c) = log(a) + log(b) + log(c)`\n",
    "<br>But again because all prob values are between 0 and 1 all log probs are gonna be negative. In terms of loss function we are actually interested in using this metric is supposing *maximazing* it (because the ideal probs are = 1, log(1) = 0 so the ideal loss value is 0). \n",
    "<br>So to make this look more as a loss function we just invert this value. This is how we get **NEGATIVE LOG-LIKELIHOOD**.\n",
    "<br>`negative log-likelihood = - log-likelihood`\n",
    "<br> and one more modification: for convinience normilized nll is used which is `NLL = NLL/n` where n is a number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9222c4f4-4794-4211-8ebf-1282ebfbdf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likelihood = 18443.9375\n",
      "normilized negative log-likelihood = 2.576688766479492\n"
     ]
    }
   ],
   "source": [
    "## training loss\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for word in words:\n",
    "    word = SPECIAL_TOKEN + word + SPECIAL_TOKEN\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        prob = P[idx1, idx2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "nll = -log_likelihood\n",
    "print(f'negative log-likelihood = {nll}')\n",
    "print(f'normilized negative log-likelihood = {nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94183494-16b4-403e-b76e-9feec560d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".p prob: 0.05, log: -2.98\n",
      "pu prob: 0.03, log: -3.44\n",
      "uz prob: 0.01, log: -5.17\n",
      "zq prob: 0.00, log: -inf\n",
      "qz prob: 0.00, log: -inf\n",
      "zl prob: 0.02, log: -3.91\n",
      "le prob: 0.16, log: -1.84\n",
      "e. prob: 0.21, log: -1.57\n",
      "negative log-likelihood = inf\n",
      "normilized negative log-likelihood = inf\n"
     ]
    }
   ],
   "source": [
    "## THIS CELL WORKS ONLY BEFORE ADDING FAKE COUNT (FOR P = N.float())\n",
    "# we can evaluate how probable any word is given parameters N of our bigram model \n",
    "## training loss\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for word in [\"puzqzle\"]: # added 'q' on purpose to get rare 'zq' pair of symbols\n",
    "    word = SPECIAL_TOKEN + word + SPECIAL_TOKEN\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        prob = P[idx1, idx2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2} prob: {prob:.2f}, log: {logprob:.2f}')\n",
    "\n",
    "nll = -log_likelihood\n",
    "print(f'negative log-likelihood = {nll}')\n",
    "print(f'normilized negative log-likelihood = {nll/n}')\n",
    "\n",
    "# oops! our loss is inf.\n",
    "# this happened because some of char pairs have 0 occurance in our training set.\n",
    "# to avoid this problem of getting log(0) we can just add fake counts to our table of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1477123-cd97-4c0e-a656-67b098160366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".p prob: 0.05, log: -2.99\n",
      "pu prob: 0.03, log: -3.43\n",
      "uz prob: 0.01, log: -4.84\n",
      "zq prob: 0.01, log: -4.36\n",
      "qz prob: 0.02, log: -3.97\n",
      "zl prob: 0.03, log: -3.66\n",
      "le prob: 0.15, log: -1.90\n",
      "e. prob: 0.20, log: -1.61\n",
      "negative log-likelihood = 26.760574340820312\n",
      "normilized negative log-likelihood = 3.345071792602539\n"
     ]
    }
   ],
   "source": [
    "## AFTER ADDING FAKE COUNT\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for word in [\"puzqzle\"]: # added 'q' on purpose to get rare 'zq' pair of symbols\n",
    "    word = SPECIAL_TOKEN + word + SPECIAL_TOKEN\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        prob = P[idx1, idx2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2} prob: {prob:.2f}, log: {logprob:.2f}')\n",
    "\n",
    "nll = -log_likelihood\n",
    "print(f'negative log-likelihood = {nll}')\n",
    "print(f'normilized negative log-likelihood = {nll/n}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pytorch_python_3_10",
   "language": "python",
   "name": "ml_pytorch_python_3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
