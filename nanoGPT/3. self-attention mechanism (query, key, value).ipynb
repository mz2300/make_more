{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f5a38fb6b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where we stopped previously\n",
    "\n",
    "# all weights are uniform and all previous tokens are treated equally\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B, T, C = 4, 8, 2 \n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0518, -0.0279,  0.1603,  0.2428,  0.0102, -0.3134,  0.0783, -0.3488],\n",
       "        [-0.1272, -0.2687, -0.3077, -0.4568,  0.3930,  0.4386, -0.2394,  1.2351],\n",
       "        [ 0.1856, -0.0142,  0.5522,  0.8337, -0.0695, -1.0379,  0.2922, -1.3450],\n",
       "        [ 0.2836, -0.0138,  0.8420,  1.2711, -0.1160, -1.5788,  0.4477, -2.0646],\n",
       "        [ 0.1334,  0.3463,  0.3064,  0.4526, -0.4921, -0.3969,  0.2602, -1.3681],\n",
       "        [-0.4107, -0.1093, -1.1863, -1.7872,  0.3284,  2.1592, -0.6664,  3.1349],\n",
       "        [ 0.0643, -0.0820,  0.2109,  0.3205,  0.0716, -0.4351,  0.0904, -0.3791],\n",
       "        [-0.2371,  0.5069, -0.8297, -1.2664, -0.5174,  1.8049, -0.3045,  1.1684]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do not want this weights to be uniform\n",
    "# because each token is gonna find other tokens more or less interesting\n",
    "# so our goal is to make this weights data dependent but not uniform\n",
    "\n",
    "# how self attention solves this problem:\n",
    "# every single token will emit 2 vectors: query and key\n",
    "# query is sort of 'what Iam I looking for'?\n",
    "# key is 'what I have to offer'? \n",
    "# value is 'what I have if you find me interesting'?\n",
    "# so my query (if I'm a specific token) dot products with all the other tokens keys \n",
    "# and this dot product becomes wei \n",
    "\n",
    "# single head of self attention\n",
    "head_size = 16\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# here all tokens independently emit their query and key\n",
    "# no token communucation happend yet\n",
    "q = query(x) # [B, T, head_size]\n",
    "k = key(x) # [B, T, head_size]\n",
    "\n",
    "# here is token communication:\n",
    "# all token queries dot product with all token keys\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # [B, T, head_size] @ [B, head_size, T] ====> [B, T, T]\n",
    "\n",
    "# head_size**-0.5 added to reduce the variance of the dot product and make it around 1\n",
    "# this normalization is important because we use softmax later\n",
    "# if the dot product is too large, softmax will focus only on the largest value\n",
    "# and instead of getting a distribution of weights, we will get a single big weight\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.3533e-01, 4.6467e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.0658e-01, 2.5107e-01, 4.4235e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6194e-01, 1.2028e-01, 2.8305e-01, 4.3472e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.8736e-01, 2.3183e-01, 2.2275e-01, 2.5782e-01, 1.0024e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.4877e-02, 7.4173e-02, 2.5266e-02, 1.3854e-02, 1.1490e-01,\n",
       "          7.1693e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [1.4379e-01, 1.2422e-01, 1.6649e-01, 1.8579e-01, 1.4484e-01,\n",
       "          8.7269e-02, 1.4759e-01, 0.0000e+00],\n",
       "         [5.7180e-02, 1.2032e-01, 3.1615e-02, 2.0428e-02, 4.3204e-02,\n",
       "          4.4064e-01, 5.3452e-02, 2.3316e-01]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.2190e-01, 2.7810e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.8724e-05, 1.7349e-03, 9.9824e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.9077e-01, 2.0902e-02, 2.3663e-04, 3.8809e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0327e-01, 2.7634e-01, 1.4466e-01, 2.9118e-01, 8.4546e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.9607e-03, 2.5382e-02, 8.5489e-01, 2.6384e-03, 5.3578e-03,\n",
       "          1.0977e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [2.5397e-01, 1.3289e-01, 2.9304e-02, 2.8447e-01, 1.1018e-01,\n",
       "          5.0143e-02, 1.3904e-01, 0.0000e+00],\n",
       "         [3.8639e-01, 6.2744e-03, 8.7213e-05, 1.5762e-01, 2.7874e-01,\n",
       "          2.2780e-03, 1.6123e-01, 7.3775e-03]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.9088e-02, 9.0091e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.5024e-01, 4.2809e-01, 2.2166e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.5372e-01, 2.0811e-02, 1.7345e-01, 5.5203e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.1793e-01, 8.0451e-02, 2.6680e-01, 3.0256e-01, 1.3225e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [8.8831e-02, 3.5959e-01, 2.2812e-01, 5.9733e-02, 8.6984e-02,\n",
       "          1.7674e-01, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2329e-01, 8.0551e-03, 5.4878e-02, 2.8211e-01, 7.0043e-02,\n",
       "          8.8015e-03, 4.5282e-01, 0.0000e+00],\n",
       "         [1.2738e-01, 3.7446e-02, 1.3452e-01, 1.8871e-01, 7.7296e-02,\n",
       "          2.3033e-02, 3.1701e-01, 9.4616e-02]],\n",
       "\n",
       "        [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.0979e-01, 2.9021e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [8.1651e-04, 1.6196e-03, 9.9756e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.2467e-03, 1.0371e-03, 1.5701e-09, 9.9372e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [6.5183e-02, 3.0858e-02, 4.3080e-05, 8.5178e-01, 5.2138e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.2260e-01, 7.1749e-02, 1.0497e-03, 6.4118e-01, 1.2586e-01,\n",
       "          3.7563e-02, 0.0000e+00, 0.0000e+00],\n",
       "         [8.6818e-02, 3.8536e-02, 1.7907e-04, 7.1614e-01, 1.3431e-01,\n",
       "          2.0156e-02, 3.8533e-03, 0.0000e+00],\n",
       "         [3.4568e-02, 1.2069e-02, 3.0271e-06, 8.8861e-01, 3.6490e-02,\n",
       "          3.3933e-03, 8.9324e-04, 2.3971e-02]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the rest is the same exept wei is not a zero matrix anymore\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0660,  0.0865, -0.0022, -0.0979,  0.0494, -0.0847, -0.1617, -0.0495,\n",
       "          0.1284,  0.1332,  0.0091,  0.0597,  0.1579, -0.0382,  0.0418, -0.0894],\n",
       "        [-0.2420,  0.1175, -0.2201, -0.1949,  0.3228,  0.1415, -0.2377,  0.0728,\n",
       "          0.0339,  0.2479,  0.1825, -0.0859,  0.2657, -0.0599, -0.1402, -0.2509],\n",
       "        [ 0.0238,  0.1610, -0.0689, -0.2005,  0.1682, -0.0809, -0.3061, -0.0503,\n",
       "          0.1969,  0.2677,  0.0678,  0.0612,  0.3090, -0.0734,  0.0190, -0.2049],\n",
       "        [ 0.2719,  0.2343,  0.0551, -0.2476,  0.0619, -0.3013, -0.4325, -0.1734,\n",
       "          0.3870,  0.3417, -0.0230,  0.2085,  0.4131, -0.1010,  0.1686, -0.2056],\n",
       "        [ 0.1616,  0.1602,  0.0217, -0.1739,  0.0611, -0.1873, -0.2971, -0.1083,\n",
       "          0.2543,  0.2386, -0.0032,  0.1304,  0.2863, -0.0697,  0.1009, -0.1501],\n",
       "        [-0.6233, -0.2353, -0.2847,  0.1831,  0.2080,  0.5735,  0.4151,  0.3220,\n",
       "         -0.5368, -0.2723,  0.2028, -0.3858, -0.3607,  0.0929, -0.3774,  0.0697],\n",
       "        [ 0.1164,  0.0891,  0.0295, -0.0917,  0.0135, -0.1246, -0.1637, -0.0714,\n",
       "          0.1526,  0.1272, -0.0154,  0.0858,  0.1550, -0.0381,  0.0718, -0.0731],\n",
       "        [-0.3887, -0.2972, -0.0986,  0.3058, -0.0446,  0.4161,  0.5462,  0.2385,\n",
       "         -0.5094, -0.4245,  0.0517, -0.2866, -0.5172,  0.1270, -0.2399,  0.2437]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x)\n",
    "#out = wei @ x\n",
    "out = wei @ v # [B, T, T] @ [B, T, head_size] ====> [B, T, head_size]\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
