{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2739adee-a929-444e-86fc-28d3d39afdc9",
   "metadata": {},
   "source": [
    "Motivation: BatchNorm was one of the first NN modification (2015) which helped `stabilize` training process of deep NN. It normalizes outputs (usually from Dense or Conv layers - layers which have multiplication operation (x*w + b) inside of it that can lead to quite extreme values) before feeding them to activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d77a8d-b9b7-4afd-8efd-9a205aeb016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5897203d-7a3a-464c-ad15-21933aaec375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique symbols: 28\n"
     ]
    }
   ],
   "source": [
    "with open('unique_english_words.txt', 'r') as f:\n",
    "    words = [word.rstrip() for word in f.readlines()] \n",
    "\n",
    "SPECIAL_TOKEN = '.'\n",
    "unique_symbols = sorted(list(set(list(''.join(words)))))\n",
    "unique_symbols.append(SPECIAL_TOKEN)\n",
    "print(f'Num of unique symbols: {len(unique_symbols)}')\n",
    "\n",
    "stoi = {s: i for i, s in enumerate(unique_symbols)}\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9324da-d4c1-4bd2-b1f8-92ee3a3a0fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 758\n",
      "val: 95\n",
      "test: 95\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(23)\n",
    "random.shuffle(words)\n",
    "\n",
    "train_split = int(0.8 * len(words))\n",
    "val_split = int(0.9 * len(words))\n",
    "\n",
    "train_words = words[:train_split]\n",
    "val_words = words[train_split:val_split]\n",
    "test_words = words[val_split:]\n",
    "\n",
    "print('train:', len(train_words))\n",
    "print('val:', len(val_words))\n",
    "print('test:', len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc572a8-fc1e-4054-8413-c65bc971cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7182, 3]) torch.Size([7182])\n",
      "torch.Size([904, 3]) torch.Size([904])\n",
      "torch.Size([931, 3]) torch.Size([931])\n"
     ]
    }
   ],
   "source": [
    "# build dataset on full data\n",
    "block_size = 3 # how many characters we are gonna use to predict the next one\n",
    "\n",
    "def build_dataset(words):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for word in words:\n",
    "        word = SPECIAL_TOKEN * block_size + word + SPECIAL_TOKEN\n",
    "        for i in range(block_size, len(word)):\n",
    "            context = word[i - block_size:i]\n",
    "            ch_to_predict = word[i]\n",
    "    \n",
    "            X.append([stoi[ch] for ch in context])\n",
    "            y.append(stoi[ch_to_predict])\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = build_dataset(train_words)\n",
    "X_val, y_val = build_dataset(val_words)\n",
    "X_test, y_test = build_dataset(test_words)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10132c6a-0cfb-4a5a-8a08-e6036bd3947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50000: 12.2216\n",
      "10000/50000: 2.5290\n",
      "20000/50000: 2.6898\n",
      "30000/50000: 2.4662\n",
      "40000/50000: 2.4155\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(23)\n",
    "\n",
    "vector_dim = 2\n",
    "C = torch.randn((len(unique_symbols), vector_dim),             generator = g)\n",
    "\n",
    "hidden_layer_size = 100\n",
    "W1 = torch.randn((block_size * vector_dim, hidden_layer_size), generator = g)\n",
    "b1 = torch.randn((hidden_layer_size,),                         generator = g)\n",
    "W2 = torch.randn((hidden_layer_size, len(unique_symbols)),     generator = g)\n",
    "b2 = torch.randn((len(unique_symbols),),                       generator = g)\n",
    "\n",
    "## add new trainable params\n",
    "bn_gain = torch.ones((1, hidden_layer_size))\n",
    "bn_bias = torch.zeros((1, hidden_layer_size))\n",
    "\n",
    "\n",
    "params = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "n_iter = 50_000\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "for i in range(n_iter):\n",
    "\n",
    "    batch_size = 64\n",
    "    rand_indecies = torch.randint(0, X_train.shape[0], (batch_size, ), generator = g)\n",
    "\n",
    "    # ---------------forward pass---------------\n",
    "\n",
    "    emb = C[X_train[rand_indecies, ...]] # shape: [batch_size, block_size, vector_dim]\n",
    "    h_preact = emb.view(-1, block_size * vector_dim) @ W1 + b1\n",
    "\n",
    "    ## BATCH NORM\n",
    "    h_preact_norm = (h_preact - h_preact.mean(dim = 0, keepdim = True)) / h_preact.std(dim = 0, keepdim = True)\n",
    "    h_scaled = bn_gain * h_preact_norm + bn_bias\n",
    "    \n",
    "    h = torch.tanh(h_scaled)\n",
    "    logits = h @ W2 + b2\n",
    "    \n",
    "    loss = F.cross_entropy(logits, y_train[rand_indecies])\n",
    "    losses_train.append(loss.item())\n",
    "    \n",
    "    # ---------------backward pass---------------\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 30_000 else 0.01\n",
    "    for p in params:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10_000 == 0:\n",
    "        print(f'{i}/{n_iter}: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe33a495-449e-49af-9e7f-8476bb63d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm in the end of training\n",
    "# (now we cannot evaluate or get prediction for a single example \n",
    "# because NN expects getting a batch to calculate its mean and std)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = C[X_train]\n",
    "    h_preact = emb.view(-1, block_size * vector_dim) @ W1 + b1\n",
    "\n",
    "    # measure mean and std of the whole X_train dataset\n",
    "    h_preact_mean = h_preact.mean(dim = 0, keepdim = True)\n",
    "    h_preact_std = h_preact.std(dim = 0, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ade4650-469d-4968-b4f4-923bb31d9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(split):\n",
    "    x, y = {'train' : [X_train, y_train], \n",
    "            'val' : [X_val, y_val], \n",
    "            'test' : [X_test, y_test]}[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    h_preact = emb.view(-1, block_size * vector_dim) @ W1 + b1\n",
    "    #h_preact_norm = (h_preact - h_preact.mean(dim = 0, keepdim = True)) / h_preact.std(dim = 0, keepdim = True)\n",
    "    h_preact_norm = (h_preact - h_preact_mean) / h_preact_std\n",
    "    h_scaled = bn_gain * h_preact_norm + bn_bias\n",
    "    h = torch.tanh(h_scaled)\n",
    "    \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2736fe-0e86-4d15-a3d7-cbc3274347b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3901)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d85edf5-aef4-4576-b8b6-b288af674d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5102)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e36316-41af-45f6-8777-73139a21313f",
   "metadata": {},
   "source": [
    "## Calculating h_preact_mean and h_preact_std during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa3a90-5c5d-4d77-a7e3-564cd6e60809",
   "metadata": {},
   "source": [
    "But instead of calculating `h_preact_mean` and `h_preact_std` after training as kind of an additional training step we can calculate both these values during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41abd8af-3a26-45c1-b7ab-925b45fd02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50000: 12.4702\n",
      "10000/50000: 2.3545\n",
      "20000/50000: 2.6113\n",
      "30000/50000: 2.2846\n",
      "40000/50000: 2.3991\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(23)\n",
    "\n",
    "vector_dim = 2\n",
    "C = torch.randn((len(unique_symbols), vector_dim),             generator = g)\n",
    "\n",
    "hidden_layer_size = 100\n",
    "W1 = torch.randn((block_size * vector_dim, hidden_layer_size), generator = g)\n",
    "#b1 = torch.randn((hidden_layer_size,),                         generator = g)\n",
    "W2 = torch.randn((hidden_layer_size, len(unique_symbols)),     generator = g)\n",
    "b2 = torch.randn((len(unique_symbols),),                       generator = g)\n",
    "\n",
    "## add new trainable params\n",
    "bn_gain = torch.ones((1, hidden_layer_size))\n",
    "bn_bias = torch.zeros((1, hidden_layer_size))\n",
    "\n",
    "bn_mean_running = torch.zeros((1, hidden_layer_size)) # in the beggining means are roughly 0 \n",
    "bn_std_running = torch.ones((1, hidden_layer_size))# and stds are roughly 1\n",
    "\n",
    "params = [C, W1, W2, b2, bn_gain, bn_bias]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "n_iter = 50_000\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "for i in range(n_iter):\n",
    "\n",
    "    batch_size = 64\n",
    "    rand_indecies = torch.randint(0, X_train.shape[0], (batch_size, ), generator = g)\n",
    "\n",
    "    # ---------------forward pass---------------\n",
    "\n",
    "    emb = C[X_train[rand_indecies, ...]] # shape: [batch_size, block_size, vector_dim]\n",
    "    h_preact = emb.view(-1, block_size * vector_dim) @ W1 #+ b1\n",
    "\n",
    "    ## BATCH NORM\n",
    "    h_preact_mean_batch = h_preact.mean(dim = 0, keepdim = True)\n",
    "    h_preact_std_batch = h_preact.std(dim = 0, keepdim = True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bn_mean_running = 0.999 * bn_mean_running + 0.001 * h_preact_mean_batch\n",
    "        bn_std_running = 0.999 * bn_std_running + 0.001 * h_preact_std_batch\n",
    "        \n",
    "    h_preact_norm = (h_preact - h_preact_mean_batch) / h_preact_std_batch\n",
    "    h_scaled = bn_gain * h_preact_norm + bn_bias\n",
    "    \n",
    "    h = torch.tanh(h_scaled)\n",
    "    logits = h @ W2 + b2\n",
    "    \n",
    "    loss = F.cross_entropy(logits, y_train[rand_indecies])\n",
    "    losses_train.append(loss.item())\n",
    "    \n",
    "    # ---------------backward pass---------------\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 30_000 else 0.01\n",
    "    for p in params:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10_000 == 0:\n",
    "        print(f'{i}/{n_iter}: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87aa662-0a55-4b48-bc9c-1db95e7a237f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3351e+00,  1.2273e+00, -2.8906e+00,  3.0119e+00, -4.5581e-01,\n",
       "         -2.8225e+00, -9.9405e-01,  4.6019e-01,  2.5960e+00,  4.0440e+00,\n",
       "          3.5605e-01,  4.9826e-01,  2.5685e+00,  2.2120e+00, -1.8808e+00,\n",
       "          2.3697e+00,  1.2427e+00, -3.1145e+00,  2.2045e+00, -1.3877e-01,\n",
       "         -1.9311e+00, -2.9764e+00,  3.5898e+00, -3.4530e-01,  3.6254e+00,\n",
       "         -1.4647e+00,  3.7733e+00,  1.4405e+00,  9.5061e-01, -2.9558e+00,\n",
       "          1.2995e+00, -2.8438e+00, -2.6863e+00,  6.8943e-01, -4.4990e-01,\n",
       "          3.2145e+00,  1.5520e+00,  2.5491e+00,  1.4607e+00,  1.2974e-01,\n",
       "          1.6177e+00,  5.1302e-01,  1.4454e+00,  3.1738e+00, -5.6435e-02,\n",
       "          2.9770e+00, -2.1937e+00, -9.7860e-01, -3.2245e+00,  3.5236e+00,\n",
       "          1.0023e+00, -9.0487e-01, -7.2602e-01, -2.5360e+00, -9.0247e-01,\n",
       "          1.4985e+00,  3.3589e+00,  3.4745e+00,  7.3291e-04,  6.8970e-01,\n",
       "         -3.0321e+00,  2.7868e+00,  1.1086e+00, -3.0644e-01,  2.3946e+00,\n",
       "         -1.3173e+00,  1.0737e+00,  3.8138e-01, -3.5657e+00, -2.0156e+00,\n",
       "          1.4100e-01, -2.3584e+00, -4.8876e+00, -2.9361e+00,  1.4309e+00,\n",
       "          1.1183e+00,  1.6405e+00, -3.8530e+00, -4.0511e+00, -7.1117e-01,\n",
       "          2.8541e+00,  3.2584e+00,  1.1348e+00, -2.5517e+00,  8.0671e-01,\n",
       "          2.0153e+00,  8.4845e-01,  1.5900e-01,  1.2025e+00,  6.8377e-01,\n",
       "          1.1045e+00, -9.2373e-01, -9.2335e-01, -2.8159e+00,  2.1194e+00,\n",
       "         -2.0451e+00,  3.6321e+00, -1.0901e+00,  1.4660e+00,  3.9104e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with the previous calculations (h_preact_mean)\n",
    "h_preact_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b348a1b-8fb6-4adb-9bd3-946624d9f864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1771e+00,  1.7308e+00, -2.4754e+00,  9.4514e-01, -9.6870e-01,\n",
       "         -2.9707e+00, -1.4417e+00, -1.1528e+00,  2.2199e+00,  1.5296e+00,\n",
       "          7.7523e-01, -1.8869e+00, -5.2390e-02,  9.1345e-01, -1.2627e+00,\n",
       "          2.4405e+00,  2.0469e+00, -2.6711e+00,  9.5091e-01, -2.2353e-02,\n",
       "         -1.4059e+00, -1.8211e+00,  1.7934e+00, -1.4754e+00,  1.1878e+00,\n",
       "         -9.7796e-01,  6.7046e-01,  2.0725e+00,  1.9278e+00, -1.0178e+00,\n",
       "          1.4137e-01, -1.2035e+00,  8.9007e-01, -9.7842e-01, -9.2034e-01,\n",
       "          2.0171e+00,  1.2965e+00,  1.0301e+00,  6.2415e-01,  7.0876e-01,\n",
       "         -1.2189e-01,  5.0645e-01,  8.0470e-01,  6.2983e-01,  6.2483e-02,\n",
       "          1.8009e+00, -8.6541e-01, -2.4259e+00,  1.8633e+00,  1.8625e+00,\n",
       "          6.2264e-01, -8.9219e-01, -8.1688e-01, -1.4510e+00, -1.3373e-01,\n",
       "         -1.6592e+00,  2.8821e+00,  2.2213e+00, -8.2503e-01,  5.8001e-01,\n",
       "         -1.2410e+00,  1.2783e+00, -7.9412e-01, -1.3878e+00, -5.3117e-01,\n",
       "          1.1548e+00,  4.5306e-01, -7.6509e-04, -4.2709e+00, -1.2114e+00,\n",
       "          4.5634e-01, -1.1033e+00, -3.6968e+00, -1.0691e+00,  1.7914e+00,\n",
       "          3.9339e-01,  2.3302e-01, -3.3414e-01, -1.4098e+00,  5.5310e-01,\n",
       "          2.5809e+00, -9.3445e-01,  1.4290e+00, -2.0780e+00,  4.8754e-01,\n",
       "          6.5231e-01, -1.3088e+00,  3.8063e-01, -7.1749e-01, -5.1520e-01,\n",
       "          2.4250e+00, -1.4045e+00, -1.5631e+00, -2.8420e+00,  3.9329e-01,\n",
       "          1.6429e-01,  3.5895e+00, -1.1032e+00,  4.6459e-01,  2.5767e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_mean_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e15b2f-15d4-4367-a642-ee932eb4217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9363, 1.7412, 3.9182, 2.8966, 2.6707, 3.2518, 1.7189, 2.2780, 3.1494,\n",
       "         3.1384, 2.0034, 2.2811, 2.5869, 2.7062, 1.8783, 2.7020, 3.0690, 2.9032,\n",
       "         2.9443, 1.9038, 2.7443, 2.6148, 2.7489, 2.6894, 3.4459, 2.2151, 2.8710,\n",
       "         2.2993, 2.7605, 2.6212, 2.8322, 3.9083, 2.9984, 1.8093, 3.1868, 2.7740,\n",
       "         3.4719, 2.2591, 2.9926, 1.9526, 2.0880, 2.4787, 3.0552, 3.1882, 2.2891,\n",
       "         3.3970, 1.9313, 2.8811, 1.7758, 4.3282, 2.1878, 2.5333, 2.3449, 2.0383,\n",
       "         3.3019, 2.3171, 3.6218, 3.5178, 1.6871, 2.6941, 2.6976, 2.5505, 2.9485,\n",
       "         1.9415, 2.6900, 1.4374, 2.1330, 2.3417, 3.2730, 1.8795, 2.3437, 2.8062,\n",
       "         3.9952, 2.3768, 2.0716, 2.4659, 2.9683, 2.7501, 3.4833, 2.1703, 2.4132,\n",
       "         2.4932, 2.4337, 2.3812, 2.5581, 1.9627, 3.2943, 1.9530, 2.4780, 2.8287,\n",
       "         2.1785, 3.3986, 2.2511, 2.8852, 3.3438, 2.1855, 4.6578, 2.7077, 2.9900,\n",
       "         3.3061]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prev std\n",
    "h_preact_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "218085a0-eca3-4bd6-a0b7-de024666ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5856, 2.4069, 4.3643, 3.0153, 2.8490, 3.7777, 2.7438, 2.7699, 3.5920,\n",
       "         2.7576, 2.4465, 1.9967, 2.1019, 2.7547, 2.6853, 3.2136, 3.8016, 3.5354,\n",
       "         2.8172, 2.2360, 2.9652, 2.0305, 2.9129, 3.2002, 2.9596, 2.4447, 2.6560,\n",
       "         2.2383, 2.7925, 2.3356, 3.0917, 3.8815, 2.1013, 1.6304, 3.4772, 3.3397,\n",
       "         3.4730, 2.3091, 3.4957, 1.8519, 1.2227, 2.3584, 3.3831, 2.9458, 2.5272,\n",
       "         3.6508, 2.0844, 3.3121, 2.7163, 4.4587, 2.4109, 2.3644, 2.7221, 2.2900,\n",
       "         3.4975, 3.1479, 4.1558, 3.9742, 2.3133, 2.6170, 2.6881, 2.1931, 3.4315,\n",
       "         2.6275, 2.2457, 1.9830, 2.2481, 2.3449, 4.2280, 2.4107, 2.8586, 3.2970,\n",
       "         4.4662, 2.1058, 2.5666, 2.8058, 3.4165, 2.2385, 3.5048, 2.2309, 2.9584,\n",
       "         2.8954, 2.7225, 2.7810, 2.5457, 2.5616, 2.8408, 1.6021, 2.4618, 3.0471,\n",
       "         2.9297, 3.9872, 2.0485, 3.3071, 3.4425, 1.7467, 5.2150, 2.6105, 3.3066,\n",
       "         3.4044]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_std_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a54bf4b-f07f-4f49-a35e-375e2de29931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update eval func as well\n",
    "@torch.no_grad()\n",
    "def eval(split):\n",
    "    x, y = {'train' : [X_train, y_train], \n",
    "            'val' : [X_val, y_val], \n",
    "            'test' : [X_test, y_test]}[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    h_preact = emb.view(-1, block_size * vector_dim) @ W1 + b1\n",
    "    h_preact_norm = (h_preact - bn_mean_running) / bn_std_running\n",
    "    h_scaled = bn_gain * h_preact_norm + bn_bias\n",
    "    h = torch.tanh(h_scaled)\n",
    "    \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368cb2cd-13b7-451e-986e-a7bdca1ffb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4249)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1685cba-da39-4121-8823-b395e505e073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4396)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f0b1bc-d45c-48bd-a7b0-a257f35052ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7462e-10, -6.9849e-10, -2.5029e-09, -6.9849e-10,  1.1642e-10,\n",
       "         0.0000e+00,  5.2387e-10, -1.8626e-09,  6.7521e-09,  1.8626e-09,\n",
       "        -9.3132e-10,  7.4506e-09, -6.5193e-09,  0.0000e+00,  2.7940e-09,\n",
       "         1.8626e-09, -2.3283e-10,  5.8208e-10, -1.8626e-09, -9.3132e-10,\n",
       "        -9.3132e-10, -3.7253e-09,  2.3283e-09, -6.0536e-09,  9.3132e-10,\n",
       "         1.8626e-09,  6.9849e-10, -9.3132e-10,  1.8626e-09,  1.8626e-09,\n",
       "        -4.6566e-09,  1.1642e-10,  1.3970e-09,  6.9849e-10, -4.6566e-10,\n",
       "         2.7940e-09,  4.3656e-11,  9.3132e-10, -4.6566e-10, -1.8626e-09,\n",
       "         5.5879e-09,  5.8208e-10,  9.3132e-10,  0.0000e+00, -1.0477e-09,\n",
       "         5.5879e-09, -8.1491e-10,  3.2596e-09,  2.3283e-10, -2.9104e-10,\n",
       "        -4.6566e-10,  0.0000e+00,  1.1642e-09,  4.6566e-10,  2.7649e-10,\n",
       "         1.5716e-09, -4.6566e-10, -1.8626e-09, -1.1642e-09, -9.3132e-10,\n",
       "        -1.8626e-09,  9.3132e-10, -3.7253e-09,  2.3283e-10, -1.8626e-09,\n",
       "        -4.6566e-10,  9.3132e-10,  1.8626e-09,  1.8626e-09,  4.6566e-10,\n",
       "         1.8626e-09,  2.9104e-10, -9.3132e-10,  1.8626e-09, -1.8626e-09,\n",
       "        -1.1642e-10,  0.0000e+00,  2.7940e-09,  2.3283e-09,  2.0955e-09,\n",
       "         1.5716e-09,  5.5879e-09, -2.7940e-09,  0.0000e+00, -2.7940e-09,\n",
       "        -2.7940e-09, -3.4925e-10, -5.5879e-09,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00, -2.3283e-10,  1.8626e-09,  2.7940e-09, -7.6252e-09,\n",
       "        -3.2596e-09,  4.6566e-10, -3.7253e-09,  0.0000e+00, -2.9104e-10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradients of b1 tend to 0 because of batch norm (whaen we calculate mean and then subtract it we basically subtract this bias)\n",
    "# so if we use batch norm bias becomes useless and we can skip it\n",
    "b1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6bad296-69f4-46cd-b3fa-2a8845a7ba4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0012, -0.0330,  0.0163, -0.0162, -0.0298, -0.0261,  0.0201, -0.0042,\n",
       "         0.0071,  0.0103,  0.0035, -0.0087,  0.0094,  0.0143,  0.0140,  0.0248,\n",
       "        -0.0177,  0.0034,  0.0271, -0.0194, -0.0123,  0.0361,  0.0077, -0.0071,\n",
       "         0.0037,  0.0264,  0.0096, -0.0605])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.grad # for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pytorch_python_3_10",
   "language": "python",
   "name": "ml_pytorch_python_3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
